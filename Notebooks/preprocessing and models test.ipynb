{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the functions for preprocessing and creating the Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Variables to be used in the process of processing and running the models\n",
    "parameters = [\n",
    "    'construction_year', 'total_area_sqm', 'nbr_frontages', 'nbr_bedrooms', 'kitchen_type_encoded',\n",
    "    'Bulding_sta_encoded', 'epc_encoded', 'garden_sqm', 'surface_land_sqm', \"fl_double_glazing\", 'fl_terrace',\n",
    "    'fl_swimming_pool', 'fl_floodzone', 'latitude', 'longitude', 'zip_code'\n",
    "]\n",
    "Heat_map_parmeters = [\n",
    "    'price', 'construction_year', 'total_area_sqm', 'nbr_frontages', 'nbr_bedrooms', 'kitchen_type_encoded',\n",
    "    'Bulding_sta_encoded', 'epc_encoded', 'garden_sqm', 'latitude', 'longitude', \"zip_code\"\n",
    "]\n",
    "\n",
    "# cleaning function\n",
    "def cleaning(dataset):\n",
    "    dataset.dropna(subset=['total_area_sqm','terrace_sqm','garden_sqm','nbr_frontages','equipped_kitchen','latitude', 'longitude'], inplace=True)\n",
    "    dataset = dataset[dataset['nbr_bedrooms'] != 0]\n",
    "    dataset['construction_year'] = dataset['construction_year'].fillna(dataset['construction_year'].mode()[0]) # ---> best results appeared with the mode vlaue\n",
    "    dataset['nbr_bedrooms'] = dataset['nbr_bedrooms'].fillna(dataset['nbr_bedrooms'].mean()) # ---> best results appeared with the mean vlaue\n",
    "    dataset['equipped_kitchen'] = dataset['equipped_kitchen'].replace(0, 'unknown').fillna('unknown') # --> filling the missing/nan/none or 0 with unknown\n",
    "    dataset['state_building'] = dataset['state_building'].replace(0, 'unknown').fillna('unknown') # --> filling the missing/nan/none or 0 with unknown\n",
    "    dataset['nbr_frontages'] = dataset['nbr_frontages'].fillna(dataset['nbr_frontages'].median())\n",
    "    dataset['epc'] = dataset['epc'].fillna(dataset['epc'].mode()[0])\n",
    "    dataset['garden_sqm'] = dataset['garden_sqm'].fillna(dataset['garden_sqm'].mean())\n",
    "    dataset['surface_land_sqm'] = dataset['surface_land_sqm'].fillna(dataset['surface_land_sqm'].median())\n",
    "    return dataset\n",
    "\n",
    "# encoding function\n",
    "def encoding(dataset):\n",
    "    kitchen_order = ['unknown','NOT_INSTALLED',\"USA_UNINSTALLED\", \"SEMI_EQUIPPED\",\"USA_SEMI_EQUIPPED\", \"INSTALLED\",\"USA_INSTALLED\",\"HYPER_EQUIPPED\",\"USA_HYPER_EQUIPPED\"] # it has to be written in Ascending order\n",
    "    building_con_order = ['unknown',\"TO_RESTORE\", \"TO_RENOVATE\", \"TO_BE_DONE_UP\", \"GOOD\", \"JUST_RENOVATED\", \"AS_NEW\"]\n",
    "    epc_order = ['G','F','E','D','C','B','A','A+','A++']\n",
    "    # Using OrdinalEncoder to encode the kitchen, energy and building status\n",
    "    encoder_kit = OrdinalEncoder(categories=[kitchen_order])\n",
    "    encoder_bul = OrdinalEncoder(categories=[building_con_order])\n",
    "    encoder_epc = OrdinalEncoder(categories=[epc_order])\n",
    "    # saving the encoded values in different columns\n",
    "    dataset['kitchen_type_encoded'] = encoder_kit.fit_transform(dataset[['equipped_kitchen']])\n",
    "    dataset['Bulding_sta_encoded'] = encoder_bul.fit_transform(dataset[['state_building']])\n",
    "    dataset['epc_encoded'] = encoder_epc.fit_transform(dataset[['epc']])\n",
    "    # Encoding the localities \n",
    "    locality_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    locality_encoded = locality_encoder.fit_transform(dataset[['locality']])\n",
    "    locality_encoded_df = pd.DataFrame(locality_encoded, columns=locality_encoder.get_feature_names_out(['locality']))\n",
    "    \n",
    "    dataset = pd.concat([dataset.reset_index(drop=True), locality_encoded_df.reset_index(drop=True)], axis=1)\n",
    "    return dataset, locality_encoded_df.columns.tolist()\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, models):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        train_score = model.score(X_train, y_train)\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mae)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"MAE\": mae, \"R²\": r2, \"Train Score\": train_score, \"Test Score\": test_score, \"Model name\": name, \"RMSE\": rmse\n",
    "        }\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T   # .T to transposes the DataFrame\n",
    "    return results_df\n",
    "\n",
    "# Define heatmap function and saving them\n",
    "def heat_map(data, title, save_path=None):\n",
    "    selected_parameters = data[Heat_map_parmeters] # Heat_map_parmeters predefined above\n",
    "    correlation_matrix = selected_parameters.corr()\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, linewidths=0.05, fmt='.2f', vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200))\n",
    "    plt.title(title)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the files and testing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing each CSV file\n",
    "directory = r\"C:\\Users\\mgabi\\Desktop\\becode\\becode_projects\\immoeliza_ml\\Preproc_ML\"\n",
    "all_results = []\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        dataset = pd.read_csv(file_path)\n",
    "        dataset, locality_encoded_columns = encoding(cleaning(dataset))\n",
    "        \n",
    "        file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "        heat_map(dataset, title=f\"Correlation map for {file_name_no_ext}\", save_path=f\"heatmap_{file_name_no_ext}.png\")\n",
    "       \n",
    "        X = dataset[parameters + locality_encoded_columns]\n",
    "        y = dataset[\"price\"]\n",
    "         # Splitting the data 80% training and 20% testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "        \n",
    "        models = {\n",
    "            \"RandomForest Regression\": RandomForestRegressor(n_estimators=1000, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=20, bootstrap=True),\n",
    "            \"XGB Regression\": XGBRegressor(n_estimators=900, subsample=0.8, min_child_weight=3, max_depth=8, learning_rate=0.05, gamma=0.2, colsample_bytree=0.36),\n",
    "            \"Hist Gradient Boosting Regression\": HistGradientBoostingRegressor(min_samples_leaf=10, max_leaf_nodes=20, max_iter=500, max_depth=10, learning_rate=0.05, l2_regularization=0.0)\n",
    "        }\n",
    "\n",
    "        results_df = evaluate_models(X_train, X_test, y_train, y_test, models)\n",
    "        results_df[\"File\"] = file_name\n",
    "        all_results.append(results_df)\n",
    "\n",
    "final_results_df = pd.concat(all_results)\n",
    "column_order = [\"File\", \"Model name\", \"MAE\", \"RMSE\", \"R²\", \"Train Score\", \"Test Score\"]\n",
    "final_results_df = final_results_df[column_order]\n",
    "print(final_results_df)\n",
    "\n",
    "final_results_df.to_csv(r\"C:\\Users\\mgabi\\Desktop\\becode\\becode_projects\\immoeliza_ml\\Preproc_ML\\Evaluation_results\\combined_model_evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
